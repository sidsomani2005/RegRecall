# RegRecall

RegRecall is a new natural language evaluation task that aims to test the efficacy of language models to identify violated SEC regulations given a textual description of actions taken. The dataset is compiled from SEC records of initiated federal civil court cases against parties accused of actions such as stock manipulation, insider trading, etc. 

We aim to develop the task to align with the legal LLM evaluation framework outlined in the [LegalBench paper](https://arxiv.org/abs/2308.11462), which is based on the [IRAC](https://en.wikipedia.org/wiki/IRAC) legal reasoning framework. In this context, RegRecall is an example of a Rule-Recall task, which involves identifying relevant regulations or laws given a description of actions taken.

Our full project proposal can be found [here](https://drive.google.com/file/d/1WBsrhjssaqYb7ydoN5JN5UlA5r5vo6vs/view?usp=drive_link).
